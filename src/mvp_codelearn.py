# -*- coding: utf-8 -*-
"""MVP_codelearn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YvCJ6nxBpeGkbX_1tu5pX8CSD1fBOrU8

## Part (a)
Import the data and save the data into a variable `X` and the targets into a variable `Y`.
"""

# Commented out IPython magic to ensure Python compatibility.
# Import suitable packages, load the dataset, and save data and targets into variables X and Y
# import packages
##TODO##
import numpy as np
import matplotlib.pyplot as plt
import json
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
# %matplotlib inline

"""Load the data

"""

#path1 = "/content/drive/MyDrive/MVP/MVPdata1.json"
path2 = "/content/drive/MyDrive/MVP/Airbus_Part_numbersML.csv"
from google.colab import drive
drive.mount('/content/drive')

# df_part = pd.read_csv(path2)
# df_part = pd.read_csv(path2, header=None, names=['Column1'])
# df_part[['Code', 'Status']] = df_part['Column1'].str.split(',', expand=True)
# df_part.drop('Column1', axis=1, inplace=True)

df_part = pd.read_csv(path2)
print(df_part)
#print(df_part.head())
print(df_part.iloc[:,0])
print(df_part.iloc[:,1])
print(df_part.shape)

# path3 = "/content/drive/MyDrive/MVP/Airbus_Part_numbersML.csv"  #save file
# df_part.to_csv(path3, index=False)

"""Encode the strings using Label Encoder for unique code."""

# encoder = LabelEncoder()
# df_part['encoded_code'] = encoder.fit_transform(df_part['Code'])  #additional column created for encoded code
# print(df_part)

X = df_part[['encoded_code']] #double brackets to ensure we have 1 column else the ML model wasnt accepting
y = df_part[['Status']]
# print(y.shape)
# print(y)
# print(X.shape)
# print(X)

"""## Part (b)

Splt data 80:20
"""

# Import the package train_test_split from sklearn.model_selection.
# Split the dataset into Xtr, Xtest, Ytr, Ytest. Xtest and Ytest will form your held-out
# test set. You will later split Xtr and Ytr into training and validation sets.
from sklearn.model_selection import train_test_split

# The function returns splits of each array passed in.
# The proportion to be used as the training set is given by test_size
Xtr, Xtest, ytr, ytest = train_test_split (X, y, test_size=0.2,random_state=10)

#print(Xtr.shape, Xtest.shape)
#print(ytr.shape, ytest.shape)
# print(ytr[0:2])
# print(Xtr[3])
# print(np.linalg.norm(Xtr[3] - Xtr[5]))
#print(len(Xtr))
print(Xtr,Xtest)
#print(ytr,ytest)
print(Xtr.shape,Xtest.shape)

"""## Part (c)
Use library, and test against training data.
"""

from sklearn.neighbors import KNeighborsClassifier   # Import KNeighborClassifier
knn = KNeighborsClassifier (n_neighbors=5)     # Instantiate the classifier with 3 neighbors
knn.fit(Xtr,ytr)        # Fit the classifier on the training data
ypred_knn_tr = knn.predict(Xtr)        #Make a prediction on the training data
print(Xtr)
print(ypred_knn_tr)
#print(ypred_knnP.shape,type(ypred_knn))

"""Check whether your predictions are the same as the predictions from `KNeighborsClassifier`."""

print("acc_knn_tr =", accuracy_score(ypred_knn_tr, ytr))  #accuracy score

ypred_knn_tst = knn.predict(Xtest) #prediction against test data
print(ypred_knn_tst)
print(Xtest.iloc[0:7])
print("acc_knn_test =", accuracy_score(ypred_knn_tst, ytest))  #accuracy score

print(knn.classes_)
print(knn.feature_names_in_)
print(knn.n_features_in_)
print(knn.n_samples_fit_)

"""Decode values"""

code_to_find = 'V52895437068-BS99'
encoded = df_part.loc[df_part['Code'] == code_to_find, 'encoded_code'].values[0]

print(f"The encoded code for {code_to_find} is {encoded}")

new_class = pd.DataFrame({'encoded_code': [42, 316, 320]})
print(new_class)

ypred_knn_new = knn.predict(new_class)
print(ypred_knn_new)

"""## Part(e) Using cross-validation for model selection and tuning

    
"""

# Use cross-validation to select the value of k
# You can use the structure below if desired

# Import KFold from sklearn.model_selection
from sklearn.model_selection import KFold
kf = KFold(n_splits=5, random_state=10, shuffle=True)   # Instantiate KFold with 5 splits, Set the parameter random_state to help you reproduce your results if needed.
max_k = 30  # Set a variable max_k to 30
train_accuracies = [[] for _ in range(max_k)]
val_accuracies = [[] for _ in range(max_k)]  # Inititalise two variables to store the training accuracies and validation accuracies, these need to store max_k*5 accuracies)
# k*5, because this will happen in the loops below, k * n_splits = 5
for k in range(0,max_k):                  # Loop over the values of k:

    knn2 = KNeighborsClassifier (n_neighbors=k+1) # Instantiate a k-nn classifier (Use the sklearn classifier) with the current value of k

    for train_index, val_index in kf.split(Xtr):
        Xtrain, Xval = Xtr[train_index], Xtr[val_index]
        ytrain, yval = ytr[train_index], ytr[val_index]   # Loop over the cross-validation splits:
        knn2 = knn2.fit(Xtrain, ytrain)    # fit the model on the current split of data
        pred_train = knn2.predict(Xtrain)
        pred_val = knn2.predict(Xval)       #Make predictions on Xtrain, Xval  (look at exercise in week 1)

        train_accuracies[k].append(accuracy_score(ytrain, pred_train)) #Calculate accuracy of predictions on training and valid set
        val_accuracies[k].append(accuracy_score(yval,pred_val))        # make predictions

# Calculate the mean training and validation accuracies across splits for each 𝑘
print(type(train_accuracies))
#train_accuracies=np.array(train_accuracies)   # no need to convert to array, axis = 1 is the issue
#print(train_accuracies.shape)
#val_accuracies=np.array(val_accuracies)
train_accuracy_mean = np.mean(train_accuracies,axis=1)  #important, note that axis = 1, since mean is in columns
#train_accuracy_stdev = np.std(train_accuracies)
val_accuracy_mean = np.mean(val_accuracies,axis=1)
#val_accuracy_stdev = np.std(val_accuracies)

# The arrays of means and standard deviation should have shape (max_k*5, ).
print(train_accuracy_mean.shape, val_accuracy_mean.shape)
print(Xtrain.shape, Xval.shape,ytrain.shape,yval.shape)

# Plot the mean training and validation accuracies against each value of k. Which value of 𝑘 will you use? Why?
# Write code to plot your results here
X = list(range(1,max_k+1))
print(X)

fig,ax = plt.subplots()
ax.plot(X, train_accuracy_mean, label = 'Training Accuracy Mean',color = 'blue')
ax.plot(X, val_accuracy_mean, label = 'Validation Accuracy Mean',color = 'orange')
plt.xticks(X)    #required to ensure x values as required
plt.xlabel('k')
plt.ylabel('accuracy')
ax.legend(loc='upper left')
#use k =8 0r 9

"""# The naive Bayes classifier - own algo (IGNORE)



"""

# Write your own implementation of naive Bayes applied to the breast cancer dataset.

# If you wish you can follow the structure below
from scipy.stats import norm

# Split the training data Xtr into training and validation sets with an 80:20 split. use the same data set obtained in part d above
# Set the random state to help with reproducibility
from sklearn.model_selection import train_test_split
print(Xtr.shape,ytr.shape,Xtest.shape,ytest.shape)     #shape of training and test used from knn above
print(Xtrain.shape, Xval.shape,ytrain.shape,yval.shape)   #shape of training and validation used from k fold above

#Separate the training set into classes, so you have one set of data for each class
Xtrain_c0 =[]
Xtrain_c1=[]
for index in range(Xtrain.shape[0]):
  if ytrain[index]==0:
    Xtrain_c0.append(Xtrain[index])
  else:
    Xtrain_c1.append(Xtrain[index])
Xtrain_c0=np.array(Xtrain_c0)
Xtrain_c1=np.array(Xtrain_c1)
np.bincount(ytrain)    #this verifies the distribution of the training data into two classes

#calculate mean and SD for each class by feature. # Calculate the means and standard deviations for each class, for each feature.
# There are 30 features in the dataset, so you should have a 30-dimensional
# array of means for each class and a 30-dimensional array of standard deviations
# for each class. Remember that you can take the average across rows or columns of
# a matrix by specifying axis = 1 or axis = 0

Xtrain_c0_mean = np.mean(Xtrain_c0,axis=0)    #axis = 0, because you want to take mean of features, which is each column, so mean of c1 = mean of feature 1, etc.
Xtrain_c1_mean = np.mean(Xtrain_c1,axis=0)
Xtrain_c0_std = np.std(Xtrain_c0,axis=0)
Xtrain_c1_std = np.std(Xtrain_c1,axis=0)
print(Xtrain_c0_mean.shape,Xtrain_c1_mean.shape,Xtrain_c0_std.shape,Xtrain_c1_std.shape)

# Calculate the prior probability p(c_i) for each class
prob_c0=Xtrain_c0.shape[0]/Xtrain.shape[0]
prob_c1=Xtrain_c1.shape[0]/Xtrain.shape[0]
print(prob_c0,prob_c1)
# Calculate the log-likelihood of each class for each datapoint in the validation set
# Hint: you can use the function scipy.stats.norm.logpdf to help with this
 #log likelihood of class 0 and 1. need to sum using axis = 1. logpdf (https://www.youtube.com/watch?v=uL4pLRvqM44) calculates each sample using mean and SD for each feature.
 # so, need to sum all features for that sample
loglike_0=np.log(prob_c0)+np.sum(norm.logpdf (Xval,Xtrain_c0_mean,Xtrain_c0_std),axis=1)
print(loglike_0.shape)
loglike_1=np.log(prob_c1)+np.sum(norm.logpdf (Xval,Xtrain_c1_mean,Xtrain_c1_std),axis=1)
print(loglike_1.shape)
a1=loglike_1 > loglike_0   #check true or false, true means class 1, false means class 0, to match the output of the prediction
print(a1)
# Your predicted class is 0 if class 0 has the highest log-likelihood, and 1 if class 1
# has the highest log-likelihood

"""## Part (b) Checking results
We now compare our results with the sklearn implementation.
"""

##Import the classifier GaussianNB from sklearn.naive_bayes
from sklearn.naive_bayes import GaussianNB
GNB = GaussianNB(var_smoothing=0.0)  # Instantiate the classifier (use the parameter var_smoothing=0.0)
GNB.fit(Xtrain,ytrain)   # fit, and predict the classes  - NOTE, need to use the same Train and val data for comparison
ypred_gnb = GNB.predict(Xval)
print(ypred_gnb)
print(ypred_gnb.shape)
print(np.argmax(ypred_gnb))
np.all(a1==ypred_gnb)      # works - matches with own algorithm!!
##TODO##

"""## Part (c) Comparing k-nearest neighbours and Gaussian naive Bayes

"""

# 1) KNN: Instantiate the knn classifer with your chosen value of k
from sklearn.neighbors import KNeighborsClassifier   # Import KNeighborClassifier
knn = KNeighborsClassifier (n_neighbors=8)     # Instantiate the classifier with 8 neighbors, as predicted from the cross-validation above
knn.fit(Xtr,ytr)        # Fit the classifier on the training data
ypred_knn_train = knn.predict(Xtr)        #Make a prediction on the training data
ypred_knn_test = knn.predict(Xtest)       #Make a prediction on the test data
#print(ypred_knn_train)
#print(ypred_knn_test)
print("acc_knn_tr =", accuracy_score(ypred_knn_train, ytr), ", acc_knn_test =", accuracy_score(ypred_knn_test, ytest) )

##Import the classifier GaussianNB from sklearn.naive_bayes
from sklearn.naive_bayes import GaussianNB
GNB = GaussianNB(var_smoothing=0.0)  # Instantiate the classifier (use the parameter var_smoothing=0.0)
GNB.fit(Xtr,ytr)   # fit, and predict the classes using original Xtr, ytr
ypred_gnb_train = GNB.predict(Xtr)
ypred_gnb_test = GNB.predict(Xtest)
print("acc_gnb_tr =", accuracy_score(ypred_gnb_train, ytr), ", acc_gnb_test =", accuracy_score(ypred_gnb_test, ytest) )
#opposite results on training and test data sets!mhow do you decide?

"""## Part (d) Using cross-validation for statistical validation
Earlier we used cross-validation to select the model parameters we would be using. We can also use it another way: to provide statistical information about which model is best. We will set up cross-validation on the whole dataset, with 10 folds.

 - Compute the accuracy for each model on the test set on each fold.
 - Calculate the mean accuracy across folds. Which model performs best?
 - Make a box-plot of the spread of scores of each model. Is there a clear difference between model performance?
 - Perform a paired t-test on the accuracy scores. What can you conclude about the performance of the two models?

"""

# Set up a k-fold cross-validation with 10 folds
##TODO##

# For each fold, fit each model on the training data
# and compute accuracy on the test data.
##TODO##

# Compute the mean and standard devation of the accuracies for each model.
# Does one model perform better?
##TODO##

# Make a boxplot of the accuracy scores. (Use plt.boxplot).
# Is there a clear difference between the models?
##TODO##

# Perform a paired t-test (you can use the function scipy.stats.ttest_rel).
# What do you conclude about the performance of the two models?
##TODO##